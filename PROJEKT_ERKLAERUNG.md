# Multi-Agent Orchestration - Vollst√§ndige Projekt-Erkl√§rung

## üìã Inhaltsverzeichnis
1. [Projekt-√úberblick](#projekt-√ºberblick)
2. [Design-Entscheidungen & Gedanken](#design-entscheidungen--gedanken)
3. [Ordnerstruktur & Was ist wo?](#ordnerstruktur--was-ist-wo)
4. [Workshop-Planung](#workshop-planung)

---

## üéØ Projekt-√úberblick

### Was haben wir gebaut?

Ein **Multi-Agent Paper Analyzer** - ein System, das wissenschaftliche Paper automatisch analysiert, indem es mehrere spezialisierte LLM-Agenten orchestriert (koordiniert).

**Kernidee:** Statt einen riesigen Prompt zu schreiben ("Analysiere dieses Paper komplett"), teilen wir die Aufgabe auf vier spezialisierte Agenten auf:
- **Reader**: Extrahiert strukturierte Notizen
- **Summarizer**: Erstellt Zusammenfassung
- **Critic**: Bewertet Qualit√§t
- **Integrator**: Erstellt finale Meta-Summary

**Besonderheit:** Wir haben den **gleichen Workflow** mit **drei verschiedenen Frameworks** implementiert, um die Unterschiede zu zeigen:
- **LangChain**: Sequenziell, einfach
- **LangGraph**: Graph-basiert, transparent
- **DSPy**: Deklarativ, selbstoptimierend

---

## üí° Design-Entscheidungen & Gedanken

### 1. Warum Multi-Agent statt einem gro√üen Prompt?

**Problem:** Ein einzelner riesiger Prompt ist schwer zu kontrollieren, zu debuggen und zu optimieren.

**L√∂sung:** Aufteilen in spezialisierte Agenten. Jeder Agent hat eine klare Aufgabe:
- **Reader** konzentriert sich nur auf Extraktion ‚Üí bessere Struktur
- **Summarizer** konzentriert sich nur auf Zusammenfassung ‚Üí bessere Lesbarkeit
- **Critic** konzentriert sich nur auf Bewertung ‚Üí objektivere Kritik
- **Integrator** kombiniert alles ‚Üí bessere Meta-Summary

**Vorteil:** Wie in einer Fabrik - jeder Arbeiter macht einen spezifischen Schritt, das Ergebnis ist besser als wenn einer alles macht.

---

### 2. Warum drei verschiedene Frameworks?

**Ziel:** Den gleichen Workflow mit unterschiedlichen Ans√§tzen zeigen, damit Teilnehmer die Trade-offs verstehen.

**LangChain (Sequenziell):**
- **Gedanke:** "Einfachheit zuerst" - f√ºr schnelle Prototypen
- **Design:** Einfache Funktionsaufrufe nacheinander
- **Vorteil:** Schnell zu verstehen, wenig Code
- **Nachteil:** Keine explizite Struktur, schwer zu erweitern

**LangGraph (Graph-basiert):**
- **Gedanke:** "Transparenz und Kontrolle" - f√ºr komplexe Workflows
- **Design:** Expliziter Graph mit Nodes und Edges
- **Vorteil:** Struktur ist sichtbar, leicht erweiterbar, Conditional Logic m√∂glich
- **Nachteil:** Mehr Code, komplexere Konzepte

**DSPy (Deklarativ):**
- **Gedanke:** "Automatische Optimierung" - f√ºr reproduzierbare Systeme
- **Design:** Signatures statt explizite Prompts
- **Vorteil:** Framework optimiert Prompts automatisch
- **Nachteil:** Weniger Kontrolle √ºber exakte Prompts

**Warum wichtig?** In der Praxis muss man entscheiden, welches Framework f√ºr welchen Use Case passt. Durch praktische Experimente versteht man die Unterschiede besser.

---

### 3. Warum Streamlit als UI?

**Gedanke:** Teilnehmer sollen sofort loslegen k√∂nnen, ohne komplexe Setup-Schritte.

**Vorteile:**
- **Einfach:** Ein Befehl startet die App
- **Interaktiv:** Upload, Buttons, Visualisierungen
- **Vergleich:** Alle drei Pipelines nebeneinander testen
- **Telemetrie:** CSV-Logging f√ºr Analyse

**Alternative w√§re:** Command-Line-Tool, aber das ist weniger zug√§nglich f√ºr Workshop-Teilnehmer.

---

### 4. Warum Text-Vorverarbeitung (utils.py)?

**Problem:** PDFs sind oft schlecht formatiert:
- Getrennte W√∂rter durch Zeilenumbr√ºche
- Metadaten (Autoren, Affiliations) am Anfang
- Literaturverzeichnis am Ende
- √úberfl√ºssige Leerzeichen

**L√∂sung:** `utils.py` normalisiert den Text:
- **`_normalize_text()`**: Behebt PDF-Probleme (getrennte W√∂rter, Leerzeichen)
- **`strip_meta_head()`**: Entfernt Metadaten am Anfang
- **`strip_references_tail()`**: Entfernt Literaturverzeichnis
- **`split_sections()`**: Erkennt Abschnitte (Abstract, Methods, Results, etc.)

**Warum wichtig?** Sauberer Text = bessere LLM-Ergebnisse. Wenn der LLM mit Metadaten und Literaturverzeichnis konfrontiert wird, verliert er Fokus.

---

### 5. Warum Telemetrie (telemetry.py)?

**Gedanke:** Metriken sammeln, um Performance zu verstehen.

**Was wird geloggt:**
- Welche Pipeline (LangChain/LangGraph/DSPy)
- Laufzeiten pro Agent
- Text-L√§ngen (Input, Summary, Meta)
- Qualit√§ts-Metriken (F1, Judge-Score)

**Warum CSV?** Einfach zu analysieren, keine externe Datenbank n√∂tig. Optional: W&B-Integration f√ºr erweiterte Visualisierung.

**Nutzen:** Teilnehmer k√∂nnen sehen, welche Pipeline schneller ist, welche bessere Ergebnisse liefert, etc.

---

### 6. Warum Doppelklick-Start (run.bat / run.sh)?

**Gedanke:** Minimale H√ºrden f√ºr Workshop-Teilnehmer.

**Was macht das Skript automatisch?**
1. Pr√ºft Python-Installation
2. Erstellt Virtual Environment (falls n√∂tig)
3. Installiert Dependencies
4. Erstellt `.env` Datei (falls n√∂tig)
5. Startet Streamlit-App

**Warum wichtig?** Teilnehmer sollen sich auf den Workshop konzentrieren, nicht auf Setup-Probleme. Ein Doppelklick = fertig.

---

### 7. Warum Quality- und Judge-Nodes nur in LangGraph?

**Gedanke:** Zeigen, dass LangGraph erweiterbarer ist.

**Quality Node:** Berechnet F1-Score zwischen Notes und Summary (automatische Metrik)

**Judge Node:** LLM-as-a-Judge bewertet Qualit√§t (0-5 Score)

**Warum nur in LangGraph?**
- In LangChain m√ºsste man sie manuell nach jedem Schritt einf√ºgen
- In LangGraph sind sie einfach Nodes im Graph - Teil der Struktur
- Zeigt: Graph-basierte Struktur macht Erweiterungen einfacher

**Lernziel:** Teilnehmer sehen, dass LangGraph nicht nur komplexer ist, sondern auch m√§chtiger.

---

### 8. Warum DSPy Teleprompting optional?

**Gedanke:** Zeigen, dass DSPy selbstoptimierend ist, aber auch Zeit kostet.

**Teleprompting:** DSPy testet verschiedene Prompts basierend auf Trainingsdaten (`eval/dev.jsonl`) und w√§hlt die beste Variante.

**Warum optional?**
- Dauert 1-2 Minuten (vs. 10-20 Sekunden ohne)
- Braucht Dev-Set (Trainingsdaten)
- Zeigt Trade-off: Bessere Ergebnisse vs. l√§ngere Laufzeit

**Lernziel:** Teilnehmer verstehen, dass Optimierung Zeit kostet, aber bessere Ergebnisse liefern kann.

---

## üìÅ Ordnerstruktur & Was ist wo?

### Root-Verzeichnis

```
multi_agent_orchestration/
‚îú‚îÄ‚îÄ agents/              # Die vier spezialisierten Agenten
‚îú‚îÄ‚îÄ workflows/           # Die drei Pipeline-Implementierungen
‚îú‚îÄ‚îÄ docs/                # Workshop-Dokumentation
‚îú‚îÄ‚îÄ eval/                # Evaluierungsdaten f√ºr DSPy
‚îú‚îÄ‚îÄ test_papers/         # Beispiel-PDFs zum Testen
‚îú‚îÄ‚îÄ venv/                # Virtual Environment (wird automatisch erstellt)
‚îú‚îÄ‚îÄ app.py               # Streamlit-Hauptanwendung
‚îú‚îÄ‚îÄ llm.py               # LLM-Konfiguration
‚îú‚îÄ‚îÄ utils.py             # Text-Vorverarbeitung
‚îú‚îÄ‚îÄ telemetry.py         # Metriken-Logging
‚îú‚îÄ‚îÄ eval_runner.py       # Evaluierungs-Skript
‚îú‚îÄ‚îÄ requirements.txt     # Python-Dependencies
‚îú‚îÄ‚îÄ run.bat              # Windows-Start-Skript
‚îú‚îÄ‚îÄ run.sh               # Mac/Linux-Start-Skript
‚îî‚îÄ‚îÄ README.md            # Projekt-√úbersicht
```

---

### `agents/` - Die vier Agenten

**Gedanke:** Jeder Agent ist eine separate Datei, damit man sie einzeln anpassen kann.

- **`reader.py`**: Extrahiert strukturierte Notizen aus Text
  - **Was macht es?** Nimmt rohen Text, gibt strukturierte Notizen zur√ºck (Title, Objective, Methods, Results, etc.)
  - **Warum so?** Klare Aufgabe, einfacher zu debuggen

- **`summarizer.py`**: Erstellt Zusammenfassung aus Notizen
  - **Was macht es?** Nimmt strukturierte Notizen, gibt 200-300 W√∂rter Summary zur√ºck
  - **Warum so?** Fokus auf Lesbarkeit, nicht auf Extraktion

- **`critic.py`**: Bewertet Summary gegen Notes
  - **Was macht es?** Pr√ºft Coherence, Groundedness, Coverage, Specificity (je 0-5)
  - **Warum so?** Objektive Bewertung durch strukturiertes Rubric

- **`integrator.py`**: Erstellt finale Meta-Summary
  - **Was macht es?** Kombiniert Notes, Summary und Critic zu finaler Meta-Summary
  - **Warum so?** Ein Ort, wo alles zusammenkommt

**Design-Entscheidung:** Alle Agenten nutzen `llm.py` f√ºr LLM-Calls. Warum? Zentrale Konfiguration - wenn man das Modell √§ndert, √§ndert man es nur einmal.

---

### `workflows/` - Die drei Pipeline-Implementierungen

**Gedanke:** Drei verschiedene Ans√§tze f√ºr den gleichen Workflow.

- **`langchain_pipeline.py`**: Sequenzielle Ausf√ºhrung
  - **Wie?** Einfache Funktionsaufrufe nacheinander
  - **Code-Struktur:** `run_reader()` ‚Üí `run_summarizer()` ‚Üí `run_critic()` ‚Üí `run_integrator()`
  - **Warum so?** Zeigt: Einfach, aber limitiert

- **`langgraph_pipeline.py`**: Graph-basierte Ausf√ºhrung
  - **Wie?** Expliziter Graph mit Nodes und Edges
  - **Code-Struktur:** `StateGraph` ‚Üí `add_node()` ‚Üí `add_edge()` ‚Üí `compile()`
  - **Zus√§tzlich:** Quality-Node (F1), Judge-Node (LLM-as-a-Judge)
  - **Warum so?** Zeigt: Komplexer, aber m√§chtiger

- **`dspy_pipeline.py`**: Deklarative Ausf√ºhrung
  - **Wie?** Signatures statt explizite Prompts
  - **Code-Struktur:** `dspy.Signature` ‚Üí `dspy.Module` ‚Üí `dspy.Predict()`
  - **Zus√§tzlich:** Optionales Teleprompting (BootstrapFewShot)
  - **Warum so?** Zeigt: Deklarativ, selbstoptimierend

**Design-Entscheidung:** Alle drei nutzen die gleichen Agent-Funktionen aus `agents/`. Warum? Zeigt, dass die Frameworks unterschiedlich orchestrieren, aber die Agenten gleich bleiben.

---

### `docs/` - Workshop-Dokumentation

**Gedanke:** Getrennte Dokumentation f√ºr Teilnehmer und Moderatoren.

```
docs/
‚îú‚îÄ‚îÄ teilnehmer/          # F√ºr Workshop-Teilnehmer
‚îÇ   ‚îú‚îÄ‚îÄ START_HIER.md           # Einstiegspunkt
‚îÇ   ‚îú‚îÄ‚îÄ TEILNEHMER_SKRIPT.md    # Hauptskript mit Aufgaben
‚îÇ   ‚îî‚îÄ‚îÄ CODE_EXPERIMENTE.md     # Code-Snippets zum Kopieren
‚îÇ
‚îî‚îÄ‚îÄ moderatoren/        # F√ºr Workshop-Moderatoren
    ‚îú‚îÄ‚îÄ WORKSHOP_LEITFADEN.md           # Zeitplan & Ablauf
    ‚îú‚îÄ‚îÄ WORKSHOP_CHECKLIST.md           # Vorbereitung
    ‚îî‚îÄ‚îÄ WORKSHOP_CODE_BEREITSTELLUNG.md # Code-Verteilung
```

**Warum getrennt?** Teilnehmer brauchen andere Infos als Moderatoren. Teilnehmer: "Wie starte ich?" Moderatoren: "Wie f√ºhre ich den Workshop durch?"

---

### `eval/` - Evaluierungsdaten

- **`dev.jsonl`**: Dev-Set f√ºr DSPy Teleprompting
  - **Format:** Jede Zeile = JSON mit `{"text": "...", "target_summary": "..."}`
  - **Warum JSONL?** Einfach zu erweitern, eine Zeile = ein Beispiel
  - **Warum wichtig?** DSPy braucht Trainingsdaten f√ºr Teleprompting

**Design-Entscheidung:** Nur kleine Beispiele im Repo. Warum? Zeigt das Konzept, aber f√ºr echte Optimierung braucht man gr√∂√üere Datens√§tze.

---

### `test_papers/` - Beispiel-PDFs

**Gedanke:** Teilnehmer sollen sofort testen k√∂nnen, ohne eigene PDFs suchen zu m√ºssen.

- Verschiedene Paper-L√§ngen (kurz, mittel, lang)
- Verschiedene Themen (LLM Agents, Graph Reasoning, DSPy, RAG)

**Warum wichtig?** Schneller Start, keine Suche nach Testdaten n√∂tig.

---

### `app.py` - Streamlit-Hauptanwendung

**Gedanke:** Zentrale UI f√ºr alle drei Pipelines.

**Struktur:**
- **Sidebar:** Settings (Model, Temperature, Max Tokens, Presets)
- **Tab 1 (Analysis):** Einzelne Pipeline ausf√ºhren
- **Tab 2 (Compare):** Alle drei Pipelines parallel vergleichen
- **Tab 3 (DSPy Optimization):** Base vs. Teleprompt vergleichen
- **Expander (Telemetry):** CSV-Logs anzeigen

**Design-Entscheidungen:**
- **Presets:** Speed/Balanced/Detail - warum? Teilnehmer sollen nicht √ºberfordert werden mit zu vielen Optionen
- **Vergleichs-Tab:** Warum? Zeigt Unterschiede direkt nebeneinander
- **Graph-Visualisierung:** Nur bei LangGraph - warum? Zeigt den Vorteil von Graph-basierter Struktur

---

### `llm.py` - LLM-Konfiguration

**Gedanke:** Zentrale Konfiguration f√ºr alle LLM-Calls.

**Was macht es?**
- L√§dt `.env` Datei (API-Key, Base-URL)
- Erstellt `ChatOpenAI` Instanz
- Konfiguriert Model, Temperature, Max Tokens, Timeout

**Warum zentral?** Alle Agenten nutzen die gleiche LLM-Instanz. Wenn man das Modell √§ndert, √§ndert man es nur einmal.

**Design-Entscheidung:** Globale Instanz statt Parameter-Weiterreichung. Warum? Einfacher zu nutzen, weniger Boilerplate.

---

### `utils.py` - Text-Vorverarbeitung

**Gedanke:** Sauberer Text = bessere LLM-Ergebnisse.

**Funktionen:**
- **`_normalize_text()`**: Behebt PDF-Probleme
- **`strip_meta_head()`**: Entfernt Metadaten
- **`strip_references_tail()`**: Entfernt Literaturverzeichnis
- **`split_sections()`**: Erkennt Abschnitte
- **`build_analysis_context()`**: Hauptfunktion - kombiniert alles

**Warum wichtig?** LLMs sind empfindlich auf Formatierung. Sauberer Text = bessere Extraktion.

---

### `telemetry.py` - Metriken-Logging

**Gedanke:** Daten sammeln f√ºr Analyse.

**Was wird geloggt?**
- Pipeline-Typ (LangChain/LangGraph/DSPy)
- Laufzeiten (gesamt, pro Agent)
- Text-L√§ngen (Input, Summary, Meta)
- Qualit√§ts-Metriken (F1, Judge-Score)

**Format:** CSV (`telemetry.csv`)

**Optional:** W&B-Integration (wenn `WANDB_ENABLED=1`)

**Warum CSV?** Einfach zu analysieren, keine externe Datenbank n√∂tig.

---

### `eval_runner.py` - Evaluierungs-Skript

**Gedanke:** Automatische Evaluierung aller drei Pipelines.

**Was macht es?**
- L√§dt `eval/dev.jsonl`
- F√ºhrt alle drei Pipelines aus
- Berechnet F1-Score f√ºr jede Pipeline
- Zeigt Vergleichstabelle

**Warum wichtig?** Objektiver Vergleich der Pipelines, nicht nur subjektive Beobachtung.

---

## üéì Workshop-Planung

### √úberblick: 60-Minuten Workshop

**Ziel:** Teilnehmer lernen drei Frameworks durch praktische Experimente kennen.

**Struktur:**
1. **0-2 Min:** Setup (App √∂ffnen)
2. **2-12 Min:** Einf√ºhrung (Konzepte verstehen)
3. **12-24 Min:** LangChain (Sequenzielle Pipeline)
4. **24-39 Min:** LangGraph (Graph-Pipeline)
5. **39-54 Min:** DSPy (Deklarative Pipeline)
6. **54-60 Min:** Vergleich & Abschluss

---

### Teil 1: Setup (0-2 Minuten)

**Was passiert?**
- Teilnehmer √∂ffnen App (Doppelklick auf `run.bat` / `run.sh`)
- App l√§dt automatisch (1-2 Minuten beim ersten Mal)
- Teilnehmer laden Test-PDF hoch

**Gedanke:** Minimale H√ºrden. Doppelklick = fertig. Keine komplexen Setup-Schritte.

**Checkliste:**
- [ ] Alle Teilnehmer haben App ge√∂ffnet
- [ ] Alle k√∂nnen PDF hochladen
- [ ] API-Key ist gesetzt (falls lokal)

---

### Teil 2: Einf√ºhrung (2-12 Minuten)

**Was passiert?**
- Moderatoren erkl√§ren Multi-Agent Orchestration
- Live-Demo: Pipeline ausf√ºhren
- Framework-√úberblick: LangChain vs. LangGraph vs. DSPy

**Gedanke:** Theorie vor Praxis, aber kurz. Teilnehmer sollen verstehen, warum man Multi-Agent nutzt.

**Lernziele:**
- Was ist Multi-Agent Orchestration?
- Warum drei Frameworks?
- Was ist der Unterschied?

---

### Teil 3: LangChain (12-24 Minuten)

**Was passiert?**
- Teilnehmer f√ºhren LangChain-Pipeline aus
- Code verstehen: `workflows/langchain_pipeline.py`
- **Experiment 1:** Prompt √§ndern (`agents/summarizer.py`)
- **Experiment 2:** Reihenfolge √§ndern (zeigt Abh√§ngigkeiten)

**Gedanke:** Zeigt Einfachheit, aber auch Limitationen.

**Lernziele:**
- Wie funktioniert sequenzielle Ausf√ºhrung?
- Wie √§ndert man Prompts?
- Was sind implizite Abh√§ngigkeiten?

**Code-Experimente:**
1. **Prompt √§ndern:** "200-300 words" ‚Üí "50-100 words"
   - **Was lernen Teilnehmer?** Prompts kontrollieren direkt das Verhalten
2. **Reihenfolge √§ndern:** Critic vor Summarizer
   - **Was lernen Teilnehmer?** Abh√§ngigkeiten sind implizit, nicht explizit

---

### Teil 4: LangGraph (24-39 Minuten)

**Was passiert?**
- Teilnehmer f√ºhren LangGraph-Pipeline aus
- Graph-Visualisierung sehen
- Code verstehen: `workflows/langgraph_pipeline.py`
- **Experiment:** Neuen Node hinzuf√ºgen (z.B. Translator)

**Gedanke:** Zeigt M√§chtigkeit von Graph-basierter Struktur.

**Lernziele:**
- Wie funktioniert Graph-basierte Orchestrierung?
- Wie f√ºgt man neue Nodes hinzu?
- Was sind Conditional Edges?

**Code-Experimente:**
1. **Graph verstehen:** Nodes und Edges sehen
   - **Was lernen Teilnehmer?** Struktur ist explizit sichtbar
2. **Node hinzuf√ºgen:** Translator zwischen Summarizer und Critic
   - **Was lernen Teilnehmer?** Erweiterung ist einfach durch Graph-Struktur

---

### Teil 5: DSPy (39-54 Minuten)

**Was passiert?**
- Teilnehmer f√ºhren DSPy-Pipeline aus
- Code verstehen: Signatures statt Prompts
- **Experiment 1:** Signature √§ndern (zeigt deklaratives Paradigma)
- **Experiment 2:** Teleprompting aktivieren (zeigt Selbstoptimierung)

**Gedanke:** Zeigt deklaratives Paradigma und Selbstoptimierung.

**Lernziele:**
- Was ist deklarativ vs. prozedural?
- Wie funktioniert Teleprompting?
- Was sind Trade-offs?

**Code-Experimente:**
1. **Signature √§ndern:** "200-300 words" ‚Üí "100-150 words"
   - **Was lernen Teilnehmer?** Beschreibung beeinflusst automatisch generierte Prompts
2. **Teleprompting:** Base vs. Optimized vergleichen
   - **Was lernen Teilnehmer?** Optimierung kostet Zeit, aber liefert bessere Ergebnisse

---

### Teil 6: Vergleich (54-60 Minuten)

**Was passiert?**
- Teilnehmer vergleichen alle drei Pipelines
- Entscheidungsmatrix ausf√ºllen
- Q&A

**Gedanke:** Zusammenfassung und Transfer.

**Lernziele:**
- Wann nutze ich welches Framework?
- Was sind die Trade-offs?

**Entscheidungsmatrix:**
| Kriterium | LangChain | LangGraph | DSPy |
|-----------|-----------|-----------|------|
| Einfachheit | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê |
| Erweiterbarkeit | ‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Transparenz | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê |
| Selbstoptimierung | ‚ùå | ‚ùå | ‚úÖ |

---

### Workshop-Materialien

**F√ºr Teilnehmer:**
- `docs/teilnehmer/START_HIER.md`: Einstiegspunkt
- `docs/teilnehmer/TEILNEHMER_SKRIPT.md`: Hauptskript mit allen Aufgaben
- `docs/teilnehmer/CODE_EXPERIMENTE.md`: Code-Snippets zum Kopieren

**F√ºr Moderatoren:**
- `docs/moderatoren/WORKSHOP_LEITFADEN.md`: Zeitplan & Ablauf
- `docs/moderatoren/WORKSHOP_CHECKLIST.md`: Vorbereitung
- `docs/moderatoren/WORKSHOP_CODE_BEREITSTELLUNG.md`: Code-Verteilung

**Gedanke:** Getrennte Dokumentation, damit jeder die richtigen Infos hat.

---

### Workshop-Vorbereitung (1 Woche vorher)

**Checkliste:**
- [ ] Code auf GitHub hochgeladen
- [ ] ZIP-Datei vorbereitet (Alternative)
- [ ] Online-App deployt (optional, aber empfohlen)
- [ ] Test-PDFs vorbereitet
- [ ] Dev-Set pr√ºfen (`eval/dev.jsonl`)
- [ ] Pr√§sentationsfolien erstellen

**Gedanke:** Vorbereitung ist wichtig. Technische Probleme w√§hrend des Workshops sind frustrierend.

---

### Workshop-Tag (30 Min vorher)

**Checkliste:**
- [ ] Streamlit-App starten und testen
- [ ] API-Key funktioniert
- [ ] Beamer/Laptop-Verbindung pr√ºfen
- [ ] Backup-Plan vorbereiten (Screenshots, vorgefertigte Ergebnisse)

**Gedanke:** Technische Probleme vorher beheben, nicht w√§hrend des Workshops.

---

### W√§hrend des Workshops

**Kommunikation:**
- Klar und in angemessenem Tempo
- Regelm√§√üig Fragen: "Hat jemand Fragen?"
- Durch den Raum gehen und unterst√ºtzen

**Code-Experimente:**
- Teilnehmer √§ndern Code direkt
- Syntax-Fehler sind h√§ufig - schnell helfen
- Code-√Ñnderungen zeigen Unterschiede besser als Theorie

**Timing:**
- Zu schnell? Vertiefte Diskussionen, Bonus-Aufgaben
- Zu langsam? K√ºrzen, Fokus auf Kernpunkte

**Gedanke:** Flexibilit√§t ist wichtig. Nicht starr am Zeitplan festhalten, sondern auf Teilnehmer reagieren.

---

### Nach dem Workshop

**Feedback sammeln:**
- Was hat gut funktioniert?
- Was war zu schwierig/einfach?
- Technische Probleme dokumentieren

**Dokumentation aktualisieren:**
- README verbessern (basierend auf Fragen)
- Code-Kommentare erweitern
- Known Issues dokumentieren

**Gedanke:** Kontinuierliche Verbesserung. Jeder Workshop ist eine Lerngelegenheit.

---

## üéØ Zusammenfassung: Was haben wir uns dabei gedacht?

### Kern-Design-Prinzipien

1. **Einfachheit f√ºr Teilnehmer:** Doppelklick-Start, klare Dokumentation, sofort loslegen k√∂nnen
2. **Vergleichbarkeit:** Gleicher Workflow, drei Frameworks ‚Üí Unterschiede werden klar
3. **Praktische Experimente:** Code √§ndern, Auswirkungen sehen ‚Üí besseres Verst√§ndnis als Theorie
4. **Transparenz:** Graph-Visualisierung, Telemetrie, klare Struktur ‚Üí Teilnehmer verstehen, was passiert
5. **Erweiterbarkeit:** Neue Nodes hinzuf√ºgen, Prompts √§ndern, Signatures anpassen ‚Üí Teilnehmer k√∂nnen experimentieren

### Warum diese Struktur?

**Ordnerstruktur:**
- **`agents/`**: Getrennte Dateien ‚Üí einzeln anpassbar
- **`workflows/`**: Drei Implementierungen ‚Üí Vergleich m√∂glich
- **`docs/`**: Getrennt f√ºr Teilnehmer/Moderatoren ‚Üí richtige Infos f√ºr jeden
- **`eval/`**: Evaluierungsdaten ‚Üí objektiver Vergleich

**Code-Struktur:**
- **Zentrale Konfiguration** (`llm.py`) ‚Üí einmal √§ndern, √ºberall wirksam
- **Wiederverwendbare Agenten** ‚Üí alle Frameworks nutzen gleiche Agenten
- **Klare Trennung** ‚Üí Agenten vs. Orchestrierung

**Workshop-Struktur:**
- **60 Minuten** ‚Üí kompakt, aber vollst√§ndig
- **Praktische Experimente** ‚Üí Code √§ndern, nicht nur zuschauen
- **Vergleich** ‚Üí Unterschiede werden klar durch Experimente

---

## üöÄ Was k√∂nnen Teilnehmer nach dem Workshop?

- **Verstehen**, was Multi-Agent Orchestration ist
- **Erkl√§ren**, wie LangChain, LangGraph und DSPy sich unterscheiden
- **Code anpassen** in allen drei Frameworks
- **Entscheiden**, welches Framework f√ºr welchen Use Case passt
- **Einen neuen Node** zu LangGraph hinzuf√ºgen
- **Signatures** in DSPy anpassen
- **Prompts** in LangChain/LangGraph √§ndern

**Das ist das Ziel:** Nicht nur Theorie, sondern praktische F√§higkeiten, die man direkt anwenden kann.

---

*Viel Erfolg beim Workshop! üéì*

